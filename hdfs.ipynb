{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6ee568",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a8d252",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c5f581",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating dataframe\n",
    "sc = SparkSession.builder.appName(\"example-spark\").getOrCreate()\n",
    "\n",
    "emp_data = [('Pavan', 1997),('Padmanabha',1997)]\n",
    "\n",
    "df = sc.createDataFrame(emp_dta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8050522",
   "metadata": {},
   "outputs": [],
   "source": [
    "#writing data into the hdfs path\n",
    "df.write.parquet(\"hdfs://example-spark-m/user/pavan/data/employee\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11340aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading data\n",
    "df1 = sc.read.parquet(\"hdfs://example-spark-m/user/pavan/data/employee\")\n",
    "\n",
    "df1.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e83a844",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking data in hadoop\n",
    "hadoop fs -ls hdfs://example-spark-m/user/pavan/data/employee\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4fb41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#deleting data from hadoop\n",
    "hdfs dfs -rm -r hdfs://example-spark-m/user/pavan/data/employee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12739c4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6050885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark Session and DataFrame creation\n",
    "sparkSession = SparkSession.builder.appName(\"example-pyspark-read-and-write\").getOrCreate()\n",
    "\n",
    "data = [('First', 1), ('Second', 2), ('Third', 3), ('Fourth', 4), ('Fifth', 5)]\n",
    "\n",
    "df = sparkSession.createDataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14227c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write into HDFS\n",
    "df.write.csv(\"hdfs://cluster/user/hdfs/test/example.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b7e02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from HDFS\n",
    "df_load = sparkSession.read.csv('hdfs://cluster/user/hdfs/test/example.csv')\n",
    "\n",
    "df_load.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
